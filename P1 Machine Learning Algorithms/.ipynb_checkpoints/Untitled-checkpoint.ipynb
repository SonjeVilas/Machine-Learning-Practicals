{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4673ce6f",
   "metadata": {},
   "source": [
    "#  Practical = 1 (Machine Learning Algorithms) \n",
    "\n",
    "## The Various Machine Learning Algorithms Are Listed Here :\n",
    "1. Linear Regression.\n",
    "2. Logistic Regression.\n",
    "3. Decision Tree.\n",
    "4. SVM (Support Vector Machine).\n",
    "5. Naive Bayes.\n",
    "6. KNN (K- Nearest Neighbors).\n",
    "7. K-Means.\n",
    "8. Random Forest.\n",
    "9. Dimensionality Reduction Algorithms.\n",
    "10. Gradient Boosting & AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc48cc",
   "metadata": {},
   "source": [
    "# 1. Linear Regression :\n",
    "\n",
    "To understand the working functionality of this algorithm, imagine how you would arrange random\n",
    "logs of wood in increasing order of their weight. There is a catch; however – you cannot weigh each\n",
    "log. You have to guess its weight just by looking at the height and girth of the log (visual analysis) and\n",
    "arrange them using a combination of these visible parameters. This is what linear regression is like.\n",
    "In this process, a relationship is established between independent and dependent variables by fitting\n",
    "them to a line. \n",
    "\n",
    "This line is known as the regression line and represented by a linear equation : Y= a *X + b.\n",
    "\n",
    "### In this equation:\n",
    "\n",
    "• Y – Dependent Variable\n",
    "\n",
    "• a – Slope\n",
    "\n",
    "• X – Independent variable\n",
    "\n",
    "• b – Intercept\n",
    "\n",
    "The coefficients a & b are derived by minimizing the sum of the squared difference of distance\n",
    "between data points and the regression line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6c2b3",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression :\n",
    "\n",
    "Logistic Regression is used to estimate discrete values (usually binary values like 0/1) from a set of\n",
    "independent variables. It helps predict the probability of an event by fitting data to a logit function. It is\n",
    "also called logit regression.\n",
    "\n",
    "### These methods listed below are often used to help improve logistic regression models :\n",
    "\n",
    "• include interaction terms\n",
    "\n",
    "• eliminate features\n",
    "\n",
    "• regularize techniques\n",
    "\n",
    "• use a non-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c444e",
   "metadata": {},
   "source": [
    "# 3. Decision Tree : (ID3 = Iterative Dichotomiser 3)\n",
    "It is one of the most popular machine learning algorithms in use today; this is a supervised learning\n",
    "algorithm that is used for classifying problems. It works well classifying for both categorical and\n",
    "continuous dependent variables. In this algorithm, we split the population into two or more\n",
    "homogeneous sets based on the most significant attributes/ independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc80a8c",
   "metadata": {},
   "source": [
    "# 4. SVM : (Support Vector Machine)\n",
    "SVM is a method of classification in which you plot raw data as points in an n-dimensional space\n",
    "(where n is the number of features you have). The value of each feature is then tied to a particular\n",
    "coordinate, making it easy to classify the data. Lines called classifiers can be used to split the data\n",
    "and plot them on a graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4ac4f",
   "metadata": {},
   "source": [
    "# 5. Naive Bayes :\n",
    "A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to\n",
    "the presence of any other feature.\n",
    "Even if these features are related to each other, a Naive Bayes classifier would consider all of these\n",
    "properties independently when calculating the probability of a particular outcome.\n",
    "A Naive Bayesian model is easy to build and useful for massive datasets. It's simple and is known to\n",
    "outperform even highly sophisticated classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9638df6",
   "metadata": {},
   "source": [
    "# 6. KNN : (K- Nearest Neighbors)\n",
    "This algorithm can be applied to both classification and regression problems. Apparently, within the\n",
    "Data Science industry, it's more widely used to solve classification problems. It’s a simple algorithm\n",
    "that stores all available cases and classifies any new cases by taking a majority vote of its k\n",
    "neighbors. The case is then assigned to the class with which it has the most in common. A distance\n",
    "function performs this measurement.\n",
    "KNN can be easily understood by comparing it to real life. For example, if you want information about\n",
    "a person, it makes sense to talk to his or her friends and colleagues!\n",
    "\n",
    "### Things to consider before selecting KNN :\n",
    "\n",
    "• KNN is computationally expensive.\n",
    "\n",
    "• Variables should be normalized, or else higher range variables can bias the algorithm.\n",
    "\n",
    "• Data still needs to be pre-processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f22da",
   "metadata": {},
   "source": [
    "# 7. K-Means :\n",
    "It is an unsupervised algorithm that solves clustering problems. Data sets are classified into a\n",
    "particular number of clusters (let's call that number K) in such a way that all the data points within a\n",
    "cluster are homogenous and heterogeneous from the data in other clusters.\n",
    "\n",
    "### How K-means forms clusters :\n",
    "\n",
    "• The K-means algorithm picks k number of points, called centroids, for each cluster.\n",
    "\n",
    "• Each data point forms a cluster with the closest centroids, i.e., K clusters.\n",
    "\n",
    "• It now creates new centroids based on the existing cluster members.\n",
    "\n",
    "• With these new centroids, the closest distance for each data point is determined. This process is\n",
    "repeated until the centroids do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e1d47",
   "metadata": {},
   "source": [
    "# 8. Random Forest :\n",
    "A collective of decision trees is called a Random Forest. To classify a new object based on its\n",
    "attributes, each tree is classified, and the tree “votes” for that class. The forest chooses the\n",
    "classification having the most votes (over all the trees in the forest).\n",
    "\n",
    "### Each tree is planted & grown as follows :\n",
    "\n",
    "• If the number of cases in the training set is N, then a sample of N cases is taken at random. This\n",
    "sample will be the training set for growing the tree.\n",
    "\n",
    "• If there are M input variables, a number m<<M is specified such that at each node, m variables are\n",
    "selected at random out of the M, and the best split on this m is used to split the node. The value of\n",
    "m is held constant during this process.\n",
    "\n",
    "• Each tree is grown to the most substantial extent possible. There is no pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9d4c6",
   "metadata": {},
   "source": [
    "# 9. Dimensionality Reduction Algorithms :\n",
    "In today's world, vast amounts of data are being stored and analyzed by corporates, government\n",
    "agencies, and research organizations. As a data scientist, you know that this raw data contains a lot\n",
    "of information - the challenge is in identifying significant patterns and variables.\n",
    "Dimensionality reduction algorithms like Decision Tree, Factor Analysis, Missing Value Ratio, and\n",
    "Random Forest can help you find relevant details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0163fd4",
   "metadata": {},
   "source": [
    "# 10. Gradient Boosting & AdaBoost : \n",
    "\n",
    "These are boosting algorithms used when massive loads of data have to be handled to make\n",
    "predictions with high accuracy. Boosting is an ensemble learning algorithm that combines the\n",
    "predictive power of several base estimators to improve robustness.\n",
    "\n",
    "In short, it combines multiple weak or average predictors to build a strong predictor. These boosting\n",
    "algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.\n",
    "These are the most preferred machine learning algorithms today. Use them, along with Python and R\n",
    "Codes, to achieve accurate outcomes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
